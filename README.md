# RAG Model README

## Overview

This project demonstrates a Retrieval-Augmented Generation (RAG) model, which is designed to integrate a large language model (LLM) with external data sources to generate precise and contextually relevant responses. By leveraging techniques such as vector embeddings and similarity search, this model can efficiently handle queries related to documentation, DDL statements, and more.

## Features

- **Question and Query Training**: Store and retrieve questions and their corresponding queries.
- **DDL Statement Training**: Train the model with Data Definition Language (DDL) statements.
- **Documentation Training**: Provide and retrieve documentation related to your data.
- **Similarity Search**: Perform similarity searches to find relevant prompts for the LLM.
- **Query Execution**: Execute tuned queries on a MongoDB server and fetch results.

## Installation

To set up the environment, you need to install the following libraries:

```bash
pip install pymongo tiktoken openai pandas chromadb uuid
```

## Usage

Here's a quick guide on how to use the main functionalities of the RAG model.

### Main Function

The main function demonstrates the overall usage of the model. Below are the key functions and their purposes:

### Training Functions

1. **Store Questions and Queries**

   Use `vector_store.add_question_nosql` to store a question along with its corresponding query.

   ```python
   vector_store.add_question_nosql(question, query)
   ```

2. **Store DDL Statements**

   Use `vector_store.add_ddl` to train the model with DDL statements.

   ```python
   vector_store.add_ddl(ddl_statement)
   ```

3. **Store Documentation**

   Use `vector_store.add_documentations` to provide documentation regarding your data.

   ```python
   vector_store.add_documentations(documentation)
   ```

### Retrieval and Execution

1. **Retrieve Relevant Prompts**

   Once you have trained the model with some questions, DDLs, and documentation, you can extract relevant prompts for your LLM.

2. **Generate and Execute Queries**

   The LLM will generate the query based on the extracted prompt. Preprocessing functions will then extract the tuned query, which can be executed on a MongoDB server to fetch results.

   ```python
   # Example pseudo-code for generating and executing a query
   prompt = nosql_generator.extract_prompt(question)
   tuned_query = llm.generate_query(prompt)
   result = nosql_generator.run_query(tuned_query)
   ```

## Example Workflow

1. **Train the Model**

   ```python
   vector_store.add_question_nosql("What is the capital of France?", "SELECT capital FROM countries WHERE name='France';")
   vector_store.add_ddl("CREATE TABLE countries (name STRING, capital STRING);")
   vector_store.add_documentations("This table stores country names and their capitals.")
   ```

2. **Retrieve Prompt and Execute Query**

   ```python
   prompt = nosql_generator.extract_prompt("What is the capital of France?")
   tuned_query = llm_Connection(prompt)
   result = nosql_generator.run_query(tuned_query)
   print(result)
   ```

## Requirements

- Python 3.6+
- MongoDB Server

## Libraries

- `pymongo`: For interacting with MongoDB.
- `tiktoken`: For tokenizing text.
- `openai`: For utilizing OpenAI's API.
- `pandas`: For data manipulation and analysis.
- `chromadb`: For vector database operations.
- `uuid`: For generating unique identifiers.

## Conclusion

This RAG model provides a robust framework for integrating an LLM with external data sources to generate and execute precise queries. By training the model with relevant questions, DDL statements, and documentation, you can enhance the accuracy and relevance of the responses generated by the LLM.
